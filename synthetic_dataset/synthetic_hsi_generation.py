# Copyright (c) 2021 Huawei Technologies Co., Ltd.
# Licensed under CC BY-NC-SA 4.0 (Attribution-NonCommercial-ShareAlike 4.0 International) (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode
#
# The code is released for academic research use only. For commercial use, please contact Huawei Technologies Co., Ltd.
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import random
import cv2
import numpy as np
import torch.nn.functional as F
from synthetic_dataset.data_format_utils import torch_to_numpy, numpy_to_torch
import math
from cv2 import imwrite
import os

def random_crop(frames, crop_sz, center_crop=False):
    """ Extract a random crop of size crop_sz from the input frames. If the crop_sz is larger than the input image size,
    then the largest possible crop of same aspect ratio as crop_sz will be extracted from frames, and upsampled to
    crop_sz.
    """
    if not isinstance(crop_sz, (tuple, list)):
        crop_sz = (crop_sz, crop_sz)
    crop_sz = torch.tensor(crop_sz).float()

    shape = frames.shape

    # Select scale_factor. Ensure the crop fits inside the image
    max_scale_factor = torch.tensor(shape[-2:]).float() / crop_sz
    max_scale_factor = max_scale_factor.min().item()

    if max_scale_factor < 1.0:
        scale_factor = max_scale_factor
    else:
        scale_factor = 1.0

    # Extract the crop
    orig_crop_sz = (crop_sz * scale_factor).floor()

    assert orig_crop_sz[-2] <= shape[-2] and orig_crop_sz[-1] <= shape[-1], 'Bug in crop size estimation!'

    if center_crop:
        r1 = (shape[-2] - orig_crop_sz[-2]) // 2
        r1 = int(r1)
        c1 = (shape[-1] - orig_crop_sz[-1]) // 2
        c1 = int(c1)
    else:
        r1 = random.randint(0, shape[-2] - orig_crop_sz[-2])
        c1 = random.randint(0, shape[-1] - orig_crop_sz[-1])

    r2 = r1 + orig_crop_sz[0].int().item()
    c2 = c1 + orig_crop_sz[1].int().item()

    frames_crop = frames[:, r1:r2, c1:c2]

    # Resize to crop_sz
    if scale_factor < 1.0:
        frames_crop = F.interpolate(frames_crop.unsqueeze(0), size=crop_sz.int().tolist(), mode='bicubic').squeeze(0)
    return frames_crop, r1, c1

def ohr2lr(image, offset=4,downsample_factor=4, interpolation_type='bilinear'):
    """ Generates a synthetic LR RAW burst from the input image. The input sRGB image is first converted to linear
    sensor space using an inverse camera pipeline. A LR burst is then generated by applying random
    transformations defined by burst_transformation_params to the input image, and downsampling it by the
    downsample_factor. The generated burst is then mosaicekd and corrputed by random noise.

    args:
        image - input sRGB image
        burst_size - Number of images in the output burst
        downsample_factor - Amount of downsampling of the input sRGB image to generate the LR image
        burst_transformation_params - Parameters of the affine transformation used to generate a burst from single image
        interpolation_type - interpolation operator used when performing affine transformations and downsampling
    """

    # Generate LR burst
    warp_hsi, flow_vectors = Subpixel_offset(image, scale_factors=offset, interpolation_type=interpolation_type)# (复位)
    #warp_hsi = warp_hsi[:, downsample_factor:-downsample_factor, downsample_factor:-downsample_factor] # (复位)
    #flow_vectors = flow_vectors[..., downsample_factor:-downsample_factor, downsample_factor:-downsample_factor] # (复位)
    warp_hsi = warp_hsi[:, offset:-offset, offset:-offset]
    flow_vectors = flow_vectors[..., offset:-offset, offset:-offset]
    image_burst,image_burst_blur,flow_vectors = oblurdown(warp_hsi, flow_vectors, downsample_factor=downsample_factor, interpolation_type=interpolation_type)

    # Add noise
    image_burst = add_gaussian_noise(image_burst)
    image_burst_blur = add_gaussian_noise(image_burst_blur)
    #image_burst = image_burst * gray_max
    # Clip saturated pixels.
    image_burst_blur = image_burst_blur.clamp(0.0, 1.0)
    image_burst = image_burst.clamp(0.0, 1.0)

    #meta_info = {'shot_noise_level': shot_noise_level, 'read_noise_level': read_noise_level}
    return image_burst, image_burst_blur, flow_vectors, image
def hr2lr(image, downsample_factor=4, interpolation_type='bilinear'):
    """ Generates a synthetic LR RAW burst from the input image. The input sRGB image is first converted to linear
    sensor space using an inverse camera pipeline. A LR burst is then generated by applying random
    transformations defined by burst_transformation_params to the input image, and downsampling it by the
    downsample_factor. The generated burst is then mosaicekd and corrputed by random noise.

    args:
        image - input sRGB image
        burst_size - Number of images in the output burst
        downsample_factor - Amount of downsampling of the input sRGB image to generate the LR image
        burst_transformation_params - Parameters of the affine transformation used to generate a burst from single image
        interpolation_type - interpolation operator used when performing affine transformations and downsampling
    """

    # Generate LR burst
    warp_hsi, flow_vectors = Subpixel_offset(image, scale_factors=downsample_factor, interpolation_type=interpolation_type)
    warp_hsi = warp_hsi[:, downsample_factor:-downsample_factor, downsample_factor:-downsample_factor]
    flow_vectors = flow_vectors[..., downsample_factor:-downsample_factor, downsample_factor:-downsample_factor]
    image_burst,flow_vectors =blurdown(warp_hsi, flow_vectors, downsample_factor=downsample_factor, interpolation_type=interpolation_type)

    # Add noise
    image_burst = add_gaussian_noise(image_burst)
    #image_burst = image_burst * gray_max
    # Clip saturated pixels.
    image_burst = image_burst.clamp(0.0, 1.0)

    #meta_info = {'shot_noise_level': shot_noise_level, 'read_noise_level': read_noise_level}
    return image_burst, flow_vectors, image

def random_noise_levels():
    """Generates random noise levels from a log-log linear distribution."""
    log_min_shot_noise = math.log(0.0001)
    log_max_shot_noise = math.log(0.012)
    log_shot_noise = random.uniform(log_min_shot_noise, log_max_shot_noise)
    shot_noise = math.exp(log_shot_noise)

    line = lambda x: 2.18 * x + 1.20
    log_read_noise = line(log_shot_noise) + random.gauss(mu=0.0, sigma=0.26)
    read_noise = math.exp(log_read_noise)
    return shot_noise, read_noise


def add_noise(image, shot_noise=0.01, read_noise=0.0005):
    """Adds random shot (proportional to image) and read (independent) noise."""
    variance = image * shot_noise + read_noise
    noise = torch.FloatTensor(image.shape).normal_().to(image.device)*variance.sqrt()
    return image + noise

def add_gaussian_noise(image, std=0.01):
    mean = torch.zeros_like(image)
    std = std * torch.ones_like(image)
    noise = torch.normal(mean, std)
    '''noise = torch.zeros_like(image)
    num_im, h, w = image.shape
    for i in range(num_im):
        mean = torch.zeros(h, w)
        std = std * torch.ones(h, w)
        noise[i, ...] = torch.normal(mean, std)'''
    return image + noise

def get_tmat(image_shape, translation, theta, shear_values, scale_factors):
    """ Generates a transformation matrix corresponding to the input transformation parameters """
    im_h, im_w = image_shape

    t_mat = np.identity(3)

    t_mat[0, 2] = translation[0]
    t_mat[1, 2] = translation[1]
    t_rot = cv2.getRotationMatrix2D((im_w * 0.5, im_h * 0.5), theta, 1.0)
    t_rot = np.concatenate((t_rot, np.array([0.0, 0.0, 1.0]).reshape(1, 3)))

    t_shear = np.array([[1.0, shear_values[0], -shear_values[0] * 0.5 * im_w],
                        [shear_values[1], 1.0, -shear_values[1] * 0.5 * im_h],
                        [0.0, 0.0, 1.0]])

    t_scale = np.array([[scale_factors[0], 0.0, 0.0],
                        [0.0, scale_factors[1], 0.0],
                        [0.0, 0.0, 1.0]])

    t_mat = t_scale @ t_rot @ t_shear @ t_mat

    t_mat = t_mat[:2, :]

    return t_mat

def Subpixel_offset(hsiimg, scale_factors=4, interpolation_type='bilinear'):
    num_spec,h,w = hsiimg.shape
    if interpolation_type == 'bilinear':
        interpolation = cv2.INTER_LINEAR
    elif interpolation_type == 'lanczos':
        interpolation = cv2.INTER_LANCZOS4
    else:
        raise ValueError

    rvs, cvs = torch.meshgrid([torch.arange(0, h),
                               torch.arange(0, w)])
    normalize = False
    if isinstance(hsiimg, torch.Tensor):
        if hsiimg.max() < 2.0:
            hsiimg = hsiimg * 255.0
            normalize = True
        hsiimg_np = torch_to_numpy(hsiimg).astype(np.uint8)

    sample_grid = torch.stack((cvs, rvs, torch.ones_like(cvs)), dim=-1).float()
    sample_pos_inv_all = []
    for i in range(num_spec):
        if i == 0:
            if normalize:
                image_t = numpy_to_torch(hsiimg_np[i, ...]).float() / 255.0
            else:
                image_t = numpy_to_torch(hsiimg_np[i, ...]).float()

            hsiimg[i, ...] = image_t
            t_mat = get_tmat((image_t.shape[0], image_t.shape[1]), (0.0, 0.0), 0.0, (0.0, 0.0), (1.0, 1.0))
            t_mat_tensor = torch.from_numpy(t_mat)
        else:
            image = hsiimg_np[i, ...]
            translation = (random.uniform(-scale_factors, scale_factors), random.uniform(-scale_factors, scale_factors))
            output_sz = (image.shape[1], image.shape[0])

            # Generate a affine transformation matrix corresponding to the sampled parameters
            t_mat = get_tmat((image.shape[0], image.shape[1]), translation, 0.0, (0.0, 0.0), (1.0, 1.0))
            t_mat_tensor = torch.from_numpy(t_mat)

            # Apply the sampled affine transformation
            image_t = cv2.warpAffine(image, t_mat, output_sz, flags=interpolation,
                                     borderMode=cv2.BORDER_CONSTANT)

            # image_t = image_t[scale_factors:-scale_factors, scale_factors:-scale_factors]
            if normalize:
                image_t = numpy_to_torch(image_t).float() / 255.0
            else:
                image_t = numpy_to_torch(image_t).float()

            hsiimg[i, ...] = image_t
        
        t_mat_tensor_3x3 = torch.cat((t_mat_tensor.float(), torch.tensor([0.0, 0.0, 1.0]).view(1, 3)), dim=0)
        t_mat_tensor_inverse = t_mat_tensor_3x3.inverse()[:2, :].contiguous()
        sample_pos_inv = torch.mm(sample_grid.view(-1, 3), t_mat_tensor_inverse.t().float()).view(
            *sample_grid.shape[:2], -1)
        sample_pos_inv_all.append(sample_pos_inv)
        # Compute the flow vectors to go from the i'th burst image to the base image

    sample_pos_inv_all = torch.stack(sample_pos_inv_all)
    flow_vectors = sample_pos_inv_all - sample_pos_inv_all[:1, ...]
    flow_vectors = flow_vectors.permute(0, 3, 1,2)
    #flow_vectors[1:,...]=flow_vectors[1:,...]-flow_vectors[:1,...].repeat(num_spec-1,1,1,1)

    return hsiimg, flow_vectors

"""
def Subpixel_offset(hsiimg, scale_factors=8, interpolation_type='bilinear'):
    num_spec, h, w = hsiimg.shape
    hsiimg_new = torch.zeros((num_spec+1, h, w))
    if interpolation_type == 'bilinear':
        interpolation = cv2.INTER_LINEAR
    elif interpolation_type == 'lanczos':
        interpolation = cv2.INTER_LANCZOS4
    else:
        raise ValueError

    rvs, cvs = torch.meshgrid([torch.arange(0, h),
                               torch.arange(0, w)])
    normalize = False
    if isinstance(hsiimg, torch.Tensor):
        if hsiimg.max() < 2.0:
            hsiimg = hsiimg * 255.0
            normalize = True
        hsiimg_np = torch_to_numpy(hsiimg).astype(np.uint8)

    sample_grid = torch.stack((cvs, rvs, torch.ones_like(cvs)), dim=-1).float()
    sample_pos_inv_all = []
    for i in range(num_spec+1):
        if i == 0:
            if normalize:
                image_t = numpy_to_torch(hsiimg_np[0, ...]).float() / 255.0
            else:
                image_t = numpy_to_torch(image_t).float()

            hsiimg_new[0, ...] = image_t
            t_mat = get_tmat((image_t.shape[0], image_t.shape[1]), (0.0, 0.0), 0.0, (0.0, 0.0), (1.0, 1.0))
            t_mat_tensor = torch.from_numpy(t_mat)
        elif i!=num_spec:
            image = hsiimg_np[i, ...]
            translation = (random.uniform(-scale_factors, scale_factors), random.uniform(-scale_factors, scale_factors))
            output_sz = (image.shape[1], image.shape[0])

            # Generate a affine transformation matrix corresponding to the sampled parameters
            t_mat = get_tmat((image.shape[0], image.shape[1]), translation, 0.0, (0.0, 0.0), (1.0, 1.0))
            t_mat_tensor = torch.from_numpy(t_mat)

            # Apply the sampled affine transformation
            image_t = cv2.warpAffine(image, t_mat, output_sz, flags=interpolation,
                                     borderMode=cv2.BORDER_CONSTANT)

            # image_t = image_t[scale_factors:-scale_factors, scale_factors:-scale_factors]
            if normalize:
                image_t = numpy_to_torch(image_t).float() / 255.0
            else:
                image_t = numpy_to_torch(image_t).float()

            hsiimg_new[i+1, ...] = image_t
        elif i==num_spec:
            image = hsiimg_np[0, ...]
            translation = (random.uniform(-scale_factors, scale_factors), random.uniform(-scale_factors, scale_factors))
            output_sz = (image.shape[1], image.shape[0])

            # Generate a affine transformation matrix corresponding to the sampled parameters
            t_mat = get_tmat((image.shape[0], image.shape[1]), translation, 0.0, (0.0, 0.0), (1.0, 1.0))
            t_mat_tensor = torch.from_numpy(t_mat)

            # Apply the sampled affine transformation
            image_t = cv2.warpAffine(image, t_mat, output_sz, flags=interpolation,
                                     borderMode=cv2.BORDER_CONSTANT)

            # image_t = image_t[scale_factors:-scale_factors, scale_factors:-scale_factors]
            if normalize:
                image_t = numpy_to_torch(image_t).float() / 255.0
            else:
                image_t = numpy_to_torch(image_t).float()

            hsiimg_new[1, ...] = image_t

        t_mat_tensor_3x3 = torch.cat((t_mat_tensor.float(), torch.tensor([0.0, 0.0, 1.0]).view(1, 3)), dim=0)
        t_mat_tensor_inverse = t_mat_tensor_3x3.inverse()[:2, :].contiguous()
        sample_pos_inv = torch.mm(sample_grid.view(-1, 3), t_mat_tensor_inverse.t().float()).view(
            *sample_grid.shape[:2], -1)
        sample_pos_inv_all.append(sample_pos_inv)
        # Compute the flow vectors to go from the i'th burst image to the base image

    sample_pos_inv_all = torch.stack(sample_pos_inv_all)
    flow_vectors = sample_pos_inv_all - sample_pos_inv_all[:1, ...]
    flow_vectors = flow_vectors.permute(0, 3, 1, 2)
    flow_vectors = flow_vectors - flow_vectors[-1:,...].repeat(num_spec+1,1,1,1)
    flow_vectors = flow_vectors[:-1,...]

    return hsiimg_new, flow_vectors
"""


def matlab_style_gauss2D(shape=(5, 5), sigma=0.5):
    """
    2D gaussian mask - should give the same result as MATLAB's
    fspecial('gaussian',[shape],[sigma])
    """
    m, n = [(ss - 1.) / 2. for ss in shape]
    y, x = np.ogrid[-m:m + 1, -n:n + 1]
    h = np.exp(-(x * x + y * y) / (2. * sigma * sigma))
    h[h < np.finfo(h.dtype).eps * h.max()] = 0
    sumh = h.sum()
    if sumh != 0:
        h /= sumh
    return h


def get_blur_kernel():
    gaussian_sigma = random.choice(
        [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0])
    #gaussian_sigma = 1.0
    gaussian_blur_kernel_size = int(math.ceil(gaussian_sigma * 3) * 2 + 1)
    kernel = matlab_style_gauss2D((gaussian_blur_kernel_size, gaussian_blur_kernel_size), gaussian_sigma)
    return kernel

def conv_func(input, kernel, padding='same'):
        b, c, h, w = input.size()
        assert b == 1, "only support b=1!"
        _, _, ksize, ksize = kernel.size()
        if padding == 'same':
            pad = ksize // 2
        elif padding == 'valid':
            pad = 0
        else:
            raise Exception("not support padding flag!")

        conv_result_list = []
        for i in range(c):
            conv_result_list.append(F.conv2d(input[:, i:i + 1, :, :], kernel, bias=None, stride=1, padding=pad))
        conv_result = torch.cat(conv_result_list, dim=1)
        return conv_result


def blur_func(x, kernel):
    b, c, h, w = x.size()
    _, kc, ksize, _ = kernel.size()
    psize = ksize // 2
    assert kc == 1, "only support kc=1!"

    # blur
    x = F.pad(x, (psize, psize, psize, psize), mode='replicate')
    blur_list = []
    for i in range(b):
        blur_list.append(conv_func(x[i:i + 1, :, :, :], kernel[i:i + 1, :, :, :]))
    blur = torch.cat(blur_list, dim=0)
    blur = blur[:, :, psize:-psize, psize:-psize]

    return blur
'''
def conv_func(input, kernel, padding='same'):
    b, c, h, w = input.size()
    _, ksize, ksize = kernel.size()
    if padding == 'same':
        pad = ksize // 2
    elif padding == 'valid':
        pad = 0
    else:
        raise Exception("not support padding flag!")

    # Apply convolution directly to all channels in the batch
    return F.conv2d(input, kernel.unsqueeze(1), bias=None, stride=1, padding=pad, groups=c)


# Blur function for batch processing
def blur_func(x, kernel):
    b, c, h, w = x.size()
    kc, ksize, _ = kernel.size()
    psize = ksize // 2

    # Pad the input to handle borders
    x = F.pad(x, (psize, psize, psize, psize), mode='replicate')

    # Directly apply convolution to all images in the batch
    blur = conv_func(x, kernel)

    # Crop the padding area
    blur = blur[:, :, psize:-psize, psize:-psize]

    return blur
'''

def blurdown(image, flow_vectors, downsample_factor=4, interpolation_type='bilinear'):
    """ Generates a burst of size burst_size from the input image by applying random transformations defined by
    transformation_params, and downsampling the resulting burst by downsample_factor.

    args:
        image - input sRGB image
        burst_size - Number of images in the output burst
        downsample_factor - Amount of downsampling of the input sRGB image to generate the LR image
        transformation_params - Parameters of the affine transformation used to generate a burst from single image
        interpolation_type - interpolation operator used when performing affine transformations and downsampling
    """
    num_spec, h, w = image.shape
    image = image.permute(1, 2, 0)
    flow_vectors = flow_vectors.reshape(2*num_spec,h,w).permute(1, 2, 0)

    if interpolation_type == 'bilinear':
        interpolation = cv2.INTER_LINEAR
    elif interpolation_type == 'lanczos':
        interpolation = cv2.INTER_LANCZOS4
    else:
        raise ValueError

    normalize = False
    flow_vectors = flow_vectors / downsample_factor
    if image.max() < 2.0:
        image = image * 255.0
        normalize = True
        if isinstance(image, torch.Tensor):
            image = torch_to_numpy(image).astype(np.uint8)

    flow_vectors = torch_to_numpy(flow_vectors)
    image_down = cv2.resize(image, None, fx=1.0 / downsample_factor, fy=1.0 / downsample_factor, interpolation=interpolation)
    flow_vectors = cv2.resize(flow_vectors, None, fx=1.0 / downsample_factor, fy=1.0 / downsample_factor,
                            interpolation=interpolation)

    if normalize:
        image_down = numpy_to_torch(image_down).float() / 255.0
    else:
        image_down = numpy_to_torch(image_down).float()

    flow_vectors = numpy_to_torch(flow_vectors)
    flow_vectors = flow_vectors.permute(2, 0, 1).reshape(num_spec,2,int(h/downsample_factor),int(w/downsample_factor))
    image_down = image_down.permute(2, 0, 1)


    return image_down, flow_vectors

def oblurdown(image, flow_vectors, downsample_factor=4, interpolation_type='bilinear'):
    """ Generates a burst of size burst_size from the input image by applying random transformations defined by
    transformation_params, and downsampling the resulting burst by downsample_factor.

    args:
        image - input sRGB image
        burst_size - Number of images in the output burst
        downsample_factor - Amount of downsampling of the input sRGB image to generate the LR image
        transformation_params - Parameters of the affine transformation used to generate a burst from single image
        interpolation_type - interpolation operator used when performing affine transformations and downsampling
    """
    num_spec, h, w = image.shape
    image = image.permute(1, 2, 0)
    flow_vectors = flow_vectors.reshape(2*num_spec,h,w).permute(1, 2, 0)

    if interpolation_type == 'bilinear':
        interpolation = cv2.INTER_LINEAR
    elif interpolation_type == 'lanczos':
        interpolation = cv2.INTER_LANCZOS4
    else:
        raise ValueError

    normalize = False
    if image.max() < 2.0:
        image = image * 255.0
        normalize = True
    flow_vectors = flow_vectors / downsample_factor

    image_blurdown = []
    for k in range(image.shape[2]):
        kernel = get_blur_kernel()
        kernel = torch.tensor(kernel, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        imgblur = blur_func(image[:,:,k].unsqueeze(0).unsqueeze(0), kernel)
        image_blurdown.append(imgblur)
    image_blurdown = torch.cat(image_blurdown, dim=1).squeeze(0)
    image_blurdown = image_blurdown.permute(1, 2, 0)
    '''
    image_blurdown = image.permute(2, 0, 1)
    kernels = []
    for k in range(image_blurdown.shape[0]):
        # Generate a random kernel for each image
        kernel = get_blur_kernel()
        kernel = torch.tensor(kernel, dtype=torch.float32).unsqueeze(0) # shape: (1, 1, kernel_size, kernel_size)
        kernels.append(kernel)

    kernels = torch.cat(kernels, dim=0)
    image_blurdown = blur_func(image_blurdown.unsqueeze(0), kernels).squeeze(0)
    image_blurdown = image_blurdown.permute(2, 0, 1)
    '''
    image_blurdown = torch_to_numpy(image_blurdown).astype(np.uint8)
    image_blurdown = cv2.resize(image_blurdown, None, fx=1.0 / downsample_factor, fy=1.0 / downsample_factor,
                                interpolation=interpolation)

    if isinstance(image, torch.Tensor):
        image = torch_to_numpy(image).astype(np.uint8)
    flow_vectors = torch_to_numpy(flow_vectors)
    image_down = cv2.resize(image, None, fx=1.0 / downsample_factor, fy=1.0 / downsample_factor, interpolation=interpolation)
    flow_vectors = cv2.resize(flow_vectors, None, fx=1.0 / downsample_factor, fy=1.0 / downsample_factor,
                            interpolation=interpolation)

    if normalize:
        image_down = numpy_to_torch(image_down).float() / 255.0
        image_blurdown = numpy_to_torch(image_blurdown).float() / 255.0
    else:
        image_down = numpy_to_torch(image_down).float()
        image_blurdown = numpy_to_torch(image_blurdown).float()

    flow_vectors = numpy_to_torch(flow_vectors)
    flow_vectors = flow_vectors.permute(2, 0, 1).reshape(num_spec,2,int(h/downsample_factor),int(w/downsample_factor))
    image_down = image_down.permute(2, 0, 1)
    image_blurdown = image_blurdown.permute(2, 0, 1)


    return image_down,image_blurdown,flow_vectors

